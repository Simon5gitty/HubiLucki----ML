{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Bilder/ost_logo.png\" width=\"240\" align=\"right\"/>\n",
    "<div style=\"text-align: left\"> <b> Machine Learning </b> <br> HS 2025 <br> <a href=\"mailto:klaus.frick@ost.ch\"> Klaus Frick </a> und <a href=\"mailto:christoph.wuersch@ost.ch\"> Christoph Würsch </a> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lektion 1\n",
    "\n",
    "In dieser Lektion erleben Sie vielleicht Ihren ersten Kontakt mit der spannenden Welt des maschinellen Lernens. Sie lernen die verschiedenen Arten von Machine Learning kennen und können anhand konkreter Beispiele die breite Anwendungsmöglichkeiten dieser Technik beurteilen. Eine sinnvolle Beschäftigung mit Machine Learning geht aber über graue Theorie hinaus und bedeutet immer **praktisches Arbeiten mit Daten und Algorithmen**. Wir werden in diesem Kurs dies mithilfe der Programmiersprache `Python` bewerkstelligen. In dieser Lektion lernen Sie die Grundzüge der Sprache kennen und tauchen ein in die Welt der Datenanalyse mit Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einführung in Python\n",
    "\n",
    "Die Programmiersprache Python wurde Ende der 80er Jahre von Guido van Rossum (Universität Amsterdam) ursprünglich für die Lehre entwickelt. Unter anderem aus diesem Grund zeichnet sich Python durch einen gut lesbaren Programmcode und eine intuitive Bedienung aus. Es handelt sich grundsätzlich um eine interpretierte Programiersprache, bei der Programmcode Zeile für Zeile von einem *interpreter* gelesen und ausgeführt wird. Dies steht z.B. im Gegensatz zu kompilierten Sprachen wie C/C++, die von einem *compiler* erst in Maschinencode übersetzt werden müssen. Python ist sehr vielseitig und *open source*, d.h. die Sprache kann von jedermann weiterentwickelt werden und der Quellcode steht offen.  \n",
    "\n",
    "Die Grundversion von Python (auf den Klassenzimmer PCs ist Python 3.6 installiert) beinhaltet lediglich die grundlegendsten Merkmale einer Programmiersprache. Es gibt z.B. nur einen sehr rudimentären Datentyp für Matrizen/Arrays im Kernmodul von Python (s. später). Die Stärke der Sprache zeigt sich vor allem durch die zahlreichen (hervorragenden) Packages, die es zu unterschiedlichen Themenbereichen gibt. Für die Data Science Community sind dies insbesondere folgende Pakete\n",
    "\n",
    "1. **NumPy** Dieses Paket bietet Funktionalitäten um effizient mit Vektoren, Matrizen und allgemein `ndarrays` umzugehen.\n",
    "2. **SciPy** bietet zahlreiche Werkzeuge für wissenschaftliches Rechnen.\n",
    "3. **Pandas** stellt das `DataFrame` Objekt zur Verfügung (eine Art abstrakte Excel Tabelle).\n",
    "4. **Matplotlib** bietet flexible Möglichkeiten zur Datenvisualisierung.\n",
    "5. **Scikit-learn** ist eine mächtige Bibliothek von machine learning Algorithmen und Data Analytics Methoden im Allgemeinen (Preprocessing, Dimensionsreduktion, ...)\n",
    "\n",
    "Diese Pakete bilden die Basis des *PyData Stacks*, einer ständig wachsenden Familie von Paketen im Umfeld von *Data Science*. \n",
    "\n",
    "<img src=\"Bilder/pydatastack.jpg\" width=\"700\" align=\"center\"/>\n",
    "\n",
    "Wir werden den Einstieg in den PyData Stack über das Paket `pandas` machen und die low-level Matrix-Bibliothek `numpy` (das Zentralmassiv wissenschaftlichen Rechnens mit Python) vorerst als gegeben annehmen und nur von Fall zu Fall genauer betrachten.\n",
    "\n",
    "Die Python Community ist gross und in den letzten Jahren stark gewachsen, sodass *crowd support* in grossem Stil vorliegt.  Python kann sehr gut mit C/C++ (Cython) oder Java (Jython) kombiniert werden und mit microPython liegt ein Python-interpreter für Microkontroller vor. Ausserdem: das **PI** in Raspberry-PI steht nicht umsonst für **P**ython **I**nterpreter. \n",
    "\n",
    "Python Tutorials und einführende Bücher gibt es wie Sand am Meer. Ich empfehle hier zwei Quellen (aus völlig subjektiver Gründen):\n",
    "\n",
    "- Intro to Python for Data Science (Video Tutorials) \n",
    "https://www.datacamp.com/courses/intro-to-python-for-data-science\n",
    "- Jake VanderPlas *A Whirlwind Tour of Python* https://github.com/jakevdp/WhirlwindTourOfPython\n",
    "\n",
    "Die Data Science community hat in den letzten Jahren Python immer mehr für sich entdeckt. Ein schönes Buch dazu ist ein treuer Begleiter von uns durch diesen Kurs:\n",
    "\n",
    "- Jake VanderPlas *Python Data Science Handbook* https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "\n",
    "Diese Quelle verwenden wir im Unterricht inflationär und für diese Lektion sollten Sie sich insbesondere Kapitel 3 des Buchs zu Gemüte führen. Sie finden das Kapitel als .pdf auf der Moodle-Plattform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython\n",
    "\n",
    "Als Programmiersprache im Sinne der Softwareentwicklung wird Python typischerweise in einer geeigneten Umgebung (IDLE, Spyder, PyCharm, Eclipse,...) entwickelt. Wir fokussieren uns aber eher auf *hacking* als auf Softwareentwicklung, sodass eine interaktive Bedienung von Python ähnlich wie in Matlab von Vorteil wäre. IPython (*Interactive Python*) bietet hier die Lösung. Wenn Sie auf Ihrem (Windows) PC Anaconda installiert haben und in der Suchleiste `IPython` eintippen bzw. dies bei unseren Klassenzimmer PCs tun, so öffnet sich eine Konsole und ein blinkender Cursor harrt Ihrer Eingabe. Dies ist die IPython Konsole und Sie können durch eintippen von Python Befehlen sequenziell Berechnungen ausführen. Im Bild unten ergibt die Eingabe `1+1` glücklicherweise `2`.\n",
    "\n",
    "<img src=\"Bilder/IPythonConsole.png\" width=\"700\" align=\"center\"/>\n",
    "\n",
    "Im typischen Data Mining/Statistik Projekten wollen Sie die Resultate und Plots Ihrer Analysen präsentieren und mit Kollegen, Kunden oder Vorgesetzten diskutieren. Dazu lässt sich IPython in Browser-basierte Notebooks einbetten, in denen neben dem ausführbaren (!) Code auch Kommentare, Formeln und Bilder gesetzt werden. Das Dokument, das Sie geraden lesen, ist so ein sogenanntes Jupyter Notebook. Bevor Sie ein Jupyter Notebook erstellen oder ein editieren können, müssen Sie im Windows Command Prompt `jupyter notebook` ausführen. \n",
    "\n",
    "Den Code in dem folgenden grauen Kasten können Sie ausführen, indem Sie hineinklicken und `Strg + Enter` drücken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'ML-venv (Python 3.13.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/simon/OneDrive - OST/ML - Machine Learning/ML-venv/Scripts/python.exe\" -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "1+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natürlich kann Python viel mehr, als einen Taschenrechner zu simulieren. Von einfacher Variablenzuweisung und dem ausführen von Scripten über die Definition von Funktionen bis hin zu objektorientierter Programmierung mit Klassen: Python bietet eine Vielzahl an Möglichkeiten. Wir konzentrieren uns hier auf die Funktionalitäten für Data Science. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eine Variablenzuweisung\n",
    "a = 1\n",
    "2*a + 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir jetzt mit Numpy und Co. starten soll hier noch einer der grossen Vorteile von IPython gegenüber einer herkömmlichen Python Programmierung erwähnt werden: Zugang zu Hilfe und Quellcode. Sie können eine Kontexthilfe zu jedem Python Objekt (Funktionen, Klassen, etc.) bekommen, indem Sie `?` an den Befehl hängen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausserdem bietet IPython Code Vervollständigung mit `<TAB>`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten darstellen mit Python\n",
    "\n",
    "Wir haben festgestellt, dass eine Datenmatrix eine ideale Struktur darstellt, um Daten für weiterführende Analysen (Machine Learning & Co) aufzubereiten (*tidy data*). Wir lernen hier, wie dies in Python gemacht werden kann. Die Datenstrukturen, die im Basispaket von Python definiert sind, sind für (grosse) Datenmengen nicht geeignet. Pandas (**Pan**el **Da**ta) ist das Paket der Wahl, wenn grosse Datenmengen mit Python eingelesen, vorverarbeitet und visualisiert werden sollen. Das Paket baut auf der mächtigen Numpy Bibliothek auf, die wir aber vorerst als gegeben betrachten wollen und nicht genauer studieren werden. Falls wir spezielle Numpy-Befehle benötigen sollten, so werden wir von Fall zu Fall darauf eingehen. \n",
    "\n",
    "### Native Python Datenstrukuren\n",
    "\n",
    "Wie oben erwähnt, verfügt der Python-Kern bereits über Datencontainer, die zwar etwas unpraktisch für Machine Learning sind, aber dennoch nicht unerwähnt bleiben sollen. \n",
    "\n",
    "- **Listen** sind die Hauptdatenstruktur im Python Basispaket. Man kann sich unter Listen ein `array` vorstellen (wie z.B. in Java), nur das Listen unterschiedlichen Objekttypen beinhalten können (Zahlen, Strings, Boole'sche Variablen, ...). Listen werden in Python mit eckigen Klammern eingegeben; die einzelnen Elemente in der Liste werden durch ein Komma getrennt. Eine Datenmatrix, wie wir sie im Machine Learning benötigen, könnten z.B. als Liste von Listen realisiert werden, wobei die einzelnen Listeneinträge die Spalten der Datenmatrix wären. Dies ist aber sehr umständlich und in der Praxis irrelevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [\"Hello\", 4.5, 2, True]\n",
    "print(myList)\n",
    "print(\" \")\n",
    "\n",
    "myDataMatrix = [[\"Hans\", \"Betty\"], [34, 23], [175.6, 167.6]]\n",
    "print(myDataMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dictionaries** sind Datencontainer, die aus key-value Paaren bestehen. Der *key* beschreibt den Namen des Eintrags und der zugehörige *value* den Wert. Die Paare werden mit der Syntax\n",
    "\n",
    "    'key-name': value\n",
    "    \n",
    "    eingegeben. Mehrere Paare werden durch ein Komma getrennt und alles zusammen in geschwungene Klammern gesetzt. Ein dictionary entspricht dem json-Datentyp, lässt sich als einfach als json-File speichern. Folgendes Beispiel zeigt, wie eine Datenmatrix (2 Elemente und 3 Merkmale) mit einem dictionary angelegt werden könnte.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict = {'Name': [\"Hans\", \"Betty\"], 'Alter': [34, 23], 'Groesse': [175.6, 167.6]}\n",
    "print(myDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beide der oben genannten Datenstrukuren haben in der Praxis grosse Bedeutung; im Maschinellen Lernen eignen sie sich aber nur bedingt, um Datenmatrizen darzustellen. Der Grund liegt hauptsächlich in der ineffizienten Datenablage intern, die von der grossen Freiheit der Verwendung von Listen bzw. Dictionaries herrührt. Wir werden nun lernen, wie die Pandas Bibliothek dieses Problem mit der Datenstruktur `DataFrame` sehr elegant löst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict['Name'].append('Sepp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDict['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a=np.array([1.9, 4.3, 7.7, 8.134, -4.3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=np.linspace(0,4*np.pi,1001)\n",
    "y=np.sin(x**2)\n",
    "plt.plot(x,y,'-')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Wer sich mit Data Science und Statistik ernsthaft auseinandersetzt, der wird feststellen, dass Daten in einer grossen Bandbreite an verschiedenen Typen vorkommen: Bilder, Soundfiles, Dokumente, Sensordaten, Zeitreihen, etc. Nach angestrengtem Hinsehen, lassen sich diese Datentypen alle als mehrdimensionale Arrays darstellen. Dies ist immer der erste Schritt in jeder Analyse. Bei gesampelten Soundfiles (1 dimensionales Array) und digitalen Bildern (2 dimensionales Array) ist das offensichtlich. Für Textdokumente müssen typischerweise erst geeignet Merkmale kodiert werden (Wortanzahl, Anzahl Sonderzeichen, Verteilung der Zahl direkt aufeinanderfolgender Grossbuchstaben, etc.). Schlussendlich ziehlt man auf eine Datenstruktur in der dritten Normalform (*tidy data*) ab, wie wir es in der Vorlesung gelernt haben.  \n",
    "\n",
    "Die Bibliothek `Pandas` baut auf `Numpy` auf und stellt die Struktur des `DataFrame` zur Verfügung (nachempfunden der gleichnamigen Struktur in `R`). Man kann sich darunter am besten ein zweidimensionales Array vorstellen, das in zweierlei Hinsicht speziell an die Bedürfnisse von Data Science angepasst wurde:\n",
    "\n",
    "1. Ein `DataFrame` kann als 2 dimensionales Array betrachtet werden, das von Spalte zu Spalte den Datentyp ändern kann (Integer, String, ...)\n",
    "2. Sowohl die Spalten- als auch die Zeilenindizierung kann frei gewählt werden (Spalten- und Zeilennamen)\n",
    "\n",
    "Man kann sich auch eine abstrakte Excel-Tabelle denken und Pandas bietet eine grose Bandbreite zur Modifikation und Verarbeitung dieser Tabellen (SQL-artige abfragen, I/O Routinen z.B. für .csv und .xls Dateien etc.) In der Sprache der Statistik bilden deie einzelnen Spalten des `DataFrame` die Mermale und die einzelnen Werte pro Zeile die Merkmalsausprägungen. Ein `DataFrame` entspricht also genau unserer Definition von *tidy data*.\n",
    "\n",
    "Wir importieren zunächst das `Pandas` Paket und nennen es um in `pd`. Wir schauen uns auch gleich die Versionsnummer des Pakets an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Paket bietet insgesamt drei fundamentale Datenstrukturen, die zur Verarbeitung von Daten verwendet werden können:\n",
    "\n",
    "- `Series`: Dabei handelt es sich um ein eindimensionales Array von Daten. Im Gegensatz zu einem herkömmlichen Array (wie z.B. in `Numpy`) sind bei  einem `Series` Objekt allerdings die Einträge *indiziert*.   \n",
    "- `DataFrame`: Wie oben beschrieben, ist ein `DataFrame` eine Verallgemeinerung eines zweidimensionales Arrays. Konkret besteht es aus zusammengehörigen `Series` Objekten, d.h. zusammengehörigen Elemente in den einzelnen `Series` Objekte habe den gleichen Index.  \n",
    "- `Index`: Es handelt sich im Wesentlichen um ein unveränderbares eindimensionales Array.\n",
    "\n",
    "\n",
    "### Series\n",
    "\n",
    "Wir erzeugen zuerst ein Series Objekt aus einer einfachen Python Liste. Wir nehmen an, dass es sich um das Alter 4 fiktiver Personen handelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.Series([25, 31, 49, 17, 41])\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie hier ersichtlich wird, hat das `Series` Objekt sowohl Werte als auch Indizes (von `0` bis `3`). Auf die Daten und die Indizes kann mit `.values` und `.index` extra zugegriffen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es kann auf die einzelnen Werte auch direkt mittels Index in eckiger Klammer zugegriffen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das schöne an `Series` Objekten (und später auch bei `DataFrames`) ist jedoch, dass sich viel allgemeinere Indizes definieren lassen. Z.B. könnten wir die Namen der Personen als Index verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.Series([25, 31, 49, 17, 41], index=['Paul', 'Sophie', 'Sepp', 'Petra', 'Jürg'])\n",
    "age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf die Einträge kann jetzt über die neuen Indizes zugegriffen werden. Hier ein Breispiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age['Sophie':'Petra']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames\n",
    "\n",
    "Ein `DataFrame` Objekt kann (u.a.) aus mehreren `Series` Objekten konstruiert werden. Wir definierne noch eine weiteres solches `Series`-Objekt und fügen es dann zu einem `DataFrame` zusammen. Dies geschieht über ein Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality = pd.Series(['Swiss', 'Swiss', 'Swiss', 'Swiss', 'Swiss'], index=['Paul',  'Sepp', 'Petra', 'Sophie', 'Jürg'])\n",
    "demo_data = pd.DataFrame({'Alter': age, 'Nationalität': nationality})\n",
    "demo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist entscheidend, dass die verwendeten `Series`-Objekte die gleiche Anzahl von Elementen besitzen. Dies ist z.B. bei dictionaries nicht nötig, weshalb diese Objekte für Machine Learning & Co zu allgemein sein. Falls ein Dictionary jedoch die korrekte Strukur hat, kann daraus leicht ein `DataFrame` erzeugt werden. Wir zeigen dies, an unserem Beispiel-Dictionary von oben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(myDict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Eingeben von Data Frames per Hand ist mühsam und spielt auch in der Praxis eine untergeordnete Rolle. Die Funktion `pd.read_csv(filename)` liest ein .csv File und verwendet die Bezeichnungen im Header (1. Zeile) als Bezeichner für die Spalten. Wir werden dazu den Datensatz `gapminder.csv` verwenden. Er stammt von dem Gapminder Projekt des schwedischen Wissenschaftlers Hans Roslin (1948-2017).\n",
    "\n",
    "https://www.gapminder.org/\n",
    "\n",
    "In dem Projekt wurden über 500 demographische Merkmale aus jedem Land der Erde aus den letzten Jahrzehnten gesammelt. Wir betrachten einen kleinen Ausschnitt davon:\n",
    "\n",
    "- `Country`: Land\n",
    "- `year`: Jahr der Datenerfassung\n",
    "- `fertility`: Anzahl der Geburten pro Frau\n",
    "- `life`: Durchschnittliche Lebenserwartung\n",
    "- `population`: Bevölkerungszahl\n",
    "- `child_mortality`: Kindersterblichkeit\n",
    "- `gdp`: Durchschnittliches Pro-Kopf-Einkommen in Dollar\n",
    "- `region`: Kontinent\n",
    "\n",
    "Der Datensatz kann mittels des `read_csv` Befehls eingelesen werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM = pd.read_csv(\"Data/gapminder.csv\")\n",
    "GM.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Befehl `head(n)` gibt die ersten n Zeilen des Datensatzes aus (default: n=5). Wir sehen z.B., dass Python die Standardindizierung (beginnend mit 0) benutzt. Später werden wir die Indizierung noch etwas sinnvoller gestalten. \n",
    "\n",
    "Bereits bei diesem einfachen Befehl wird ein Grundprinzip der Python-Sprache bereits sehr offensichtlich: **Objektorientierung**. Konkret bedeutet dies, dass `GM` nicht einfach nur eine Tabelle ist, sondern eine Klasse mit zahlreichen *Methoden*. Diese Methoden sind Funktionen, die sich auf das Datenfeld in `GM` beziehen. Beispielsweise macht der Befehl\n",
    "\n",
    "`GM.head()`\n",
    "\n",
    "nichts anderes, als in seinem eigenen Datenfeld die ersten 5 Zeilen zu holen und (ansprechend) auszugeben. Es gibt extrem viele Methoden in der `DataFrame`-Klasse, die zur Zusammenfassung, Beschreibung, Transformation und Visualisierung der Daten genutzt werden können. Wir werden nach und nach einige dieser Funktionen kennenlernen. \n",
    "\n",
    "Meistens beginnt man damit, die Grösse des Datenframes auszugeben. Dies geschieht mit dem Aufruf `shape`. Es handelt sich dabei nicht um eine Methode, sondern um ein Datenfeld (2-Tuple), weshalb keine Klammern gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir haben also 10111 Elemente in unserer Stichprobe und 8 Merkmale. Sehr praktisch ist der `.describe()` Befehl, der eine Zusammenfassung der quantitativen Merkmale im Datensatz gibt (beim Merkmal `Year` macht dies ev. wenig Sinn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auswahl von Elementen\n",
    "\n",
    "Die Indizierung von `DataFrame` Objekten ist sehr viellseitig und eine komplete Ausführung würde hier den Rahmen sprengen. Wir verweisen dafür auf Kapitel 3 im Buch von Jake VanderPlas *Python für Data Science*. Hier seien die grundlegenden Verfahren beschrieben. Die Indizierung kann analog zu einem `Numpy`-Array erfolgen. Hier ist zu beachten, dass der letzte Index *nicht* mehr ausgegeben wird, wie folgendes Beispiel eines *slices* zeigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM[20:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der `DataFrame` im obigen Beispiel hat eine etwas eigentümliche Struktur im Sinne des Maschinellen Lernens. Die Ausprägungen des Merkmals `Country` sind streckenweise konstant, wobei jeweils Werte in verschiedenen Jahren angegeben werden. Rein formal entspricht der Datensatz so der gewünschten Form von *tidy data*, aber eigentlich sind `Country` und `Year` keine Merkmale, sondern *Indizes*. \n",
    "\n",
    "Wir machen dies deutlich, in dem wir zunächst die Daten eines Jahres auswählen. Dies kann einerseits über *logische* Bedingungen erfolgen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_1980 = GM[GM['Year']==1980]\n",
    "GM_1980.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klarerweise ist jetzt das Merkmal `Year` überflüssig. Wir entfernen die Spalte mit `drop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_1980 = GM_1980.drop('Year', axis=1)\n",
    "GM_1980.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Name des Landes ist eher ein Index als ein Merkmal (technisch gesehen ist der Name `Afghanistan` ein Merkmal des Landes Afghanistan, aber es beinhaltet keine statistisch relevante Information). Wir setzen also den Index auf den Namen des Landes (wir gehen davon aus, dass der Name eindeutig ist, d.h. das es nicht zwei unterschiedliche Länder mit dem Namen Afghanistan gibt o.ä.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_1980.set_index('Country', inplace=True, drop=True)\n",
    "GM_1980.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das sieht doch schon einmal besser aus! Das selbe Resultat hätten wir auch mit dem mächtigen `groupby`-Befehl erreicht. Er gruppiert den Datensatz nach den Werten eines Merkmals. Mit dem Befehl `get_group` kann denn jede gewünschte Gruppe extrahiert werden.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_1980_v1 = GM.groupby('Year').get_group(1980)\n",
    "GM_1980 = GM_1980_v1.drop('Year', axis=1).set_index('Country', drop=True)\n",
    "GM_1980.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auf die gruppierten Variabeln können jetzt z.B. statistisch Merkmale angewendet werden. Wir führen dies am Beispiel des Mittelwerts vor. Wir gruppieren den Datensatz `GM_1980` nach dem Kontinent (`region`) und berechnen den Mittelwert in den Gruppen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GM_1980_v1.groupby('region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit dem Datensatz können nun erste Visualisierungen durchgeführt werden, wobei wir in der nächsten Lektion diese wichtige Werkzeug der Datenanalyse vertieft kennen lernen werden. Hier beginnen wir mit einem `Scatter-Plot`, den wir mit dem Paket `seaborn` erzeugen (dazu später mehr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lmplot(x=\"fertility\", y=\"life\", hue=\"region\", data = GM_1980, fit_reg=False)\n",
    "plt.title(\"Fruchtbarkeit vs. Lebenserwartung\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
